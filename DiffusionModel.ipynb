{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOF+pZOaD7KlrDRkQYlsDLN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samuel23taku/NoteBooks/blob/main/DiffusionModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading"
      ],
      "metadata": {
        "id": "GH9BhSmDtyzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets,transforms\n",
        "from torchvision.transforms import  ToTensor, Normalize\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "# Transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,),(0.5,))\n",
        "])\n",
        "\n",
        "# Other code\n",
        "training_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "testing_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    training_data, batch_size = 2000,shuffle=True\n",
        ")\n",
        "test_dataloader = DataLoader(\n",
        "    testing_data, batch_size = 1000,shuffle=True\n",
        ")\n",
        "\n",
        "\n",
        "train_features,train_labels = next(iter(train_dataloader))\n",
        "eg_image = train_features[0].squeeze()\n",
        "label = train_labels[0]\n",
        "plt.imshow(eg_image)\n",
        "plt.title(label)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "-6_bqgV8bQS9",
        "outputId": "61286c6a-af73-496a-d458-93c1242c4ecf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hRnYYw5RfDZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Diffusion Process\n",
        "\n"
      ],
      "metadata": {
        "id": "tDXoa2VVtqjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Util_DoubleConv(nn.Module):\n",
        "  def __init__(self,in_channels,out_channels,mid_channels=None): # mid_channels for number of neurons in the middle layer\n",
        "    super().__init__()\n",
        "    if not mid_channels:\n",
        "        mid_channels = out_channels\n",
        "    self.double_conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
        "        nn.BatchNorm2d(mid_channels),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.double_conv(x)\n",
        "\n",
        "class Util_DownScale(nn.Module):\n",
        "  def __init__(self,in_channels,out_channels):\n",
        "    super().__init__()\n",
        "    self.maxpool_conv = nn.Sequential(\n",
        "        nn.MaxPool2d(kernel_size=2),\n",
        "        Util_DoubleConv(in_channels,out_channels)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.maxpool_conv(x)\n",
        "\n",
        "class Util_UpScale(nn.Module):\n",
        "  def __init__(self,in_channels,out_channels,bilinear=True): # Either use bilinear or transposed Conv2d\n",
        "    super().__init__()\n",
        "    if bilinear:\n",
        "      self.up = nn.Upsample(scale_factor=2,mode=\"bilinear\",align_corners=True)\n",
        "      self.conv = Util_DoubleConv(in_channels=in_channels,out_channels=out_channels,mid_channels= in_channels // 2)\n",
        "    else:\n",
        "      self.up = nn.ConvTranspose2d(in_channels=in_channels,out_channels=out_channels,kernel_size = 2,stride=2)\n",
        "      self.conv = Util_DoubleConv(in_channels,out_channels,mid_channels= in_channels // 2)\n",
        "\n",
        "  def forward(self,x1,x2):\n",
        "    '''\n",
        "        x2 is a skip connection value from the encoder,\n",
        "        while x1 is the decoder current state\n",
        "    '''\n",
        "\n",
        "    x1 = self.up(x1)\n",
        "\n",
        "    # Compute height difference between x1 and x2\n",
        "    diffY = x2.size()[2] - x1.size()[2]\n",
        "    diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "    # Make x1 match the size of x2\n",
        "    x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,diffY // 2, diffY - diffY // 2])\n",
        "\n",
        "    # Do skip connection by concaticating tensor from different layers\n",
        "    x = torch.cat([x2,x1],dim=1)\n",
        "\n",
        "    return self.conv(x)\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class DiffusionUNet(nn.Module):\n",
        "    def __init__(self, in_channels, n_classes, bilinear=False):\n",
        "        super(DiffusionUNet, self).__init__()\n",
        "        self.n_channels = in_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.bilinear = bilinear\n",
        "\n",
        "        self.inc = (Util_DoubleConv(in_channels, 64))\n",
        "        self.down1 = (Util_DownScale(64, 128))\n",
        "        self.down2 = (Util_DownScale(128, 256))\n",
        "        self.down3 = (Util_DownScale(256, 512))\n",
        "        factor = 2 if bilinear else 1\n",
        "        self.down4 = (Util_DownScale(512, 1024 // factor))\n",
        "        self.up1 = (Util_UpScale(1024, 512 // factor, bilinear))\n",
        "        self.up2 = (Util_UpScale(512, 256 // factor, bilinear))\n",
        "        self.up3 = (Util_UpScale(256, 128 // factor, bilinear))\n",
        "        self.up4 = (Util_UpScale(128, 64, bilinear))\n",
        "        self.outc = (OutConv(64, n_classes))\n",
        "\n",
        "    def forward(self, x):\n",
        "      x1 = self.inc(x)\n",
        "      x2 = self.down1(x1)\n",
        "      x3 = self.down2(x2)\n",
        "      x4 = self.down3(x3)\n",
        "      x5 = self.down4(x4)\n",
        "      x = self.up1(x5, x4)\n",
        "      x = self.up2(x, x3)\n",
        "      x = self.up3(x, x2)\n",
        "      x = self.up4(x, x1)\n",
        "      logits = self.outc(x)\n",
        "      return logits\n",
        "\n",
        "    def use_checkpointing(self):\n",
        "      self.inc = torch.utils.checkpoint(self.inc)\n",
        "      self.down1 = torch.utils.checkpoint(self.down1)\n",
        "      self.down2 = torch.utils.checkpoint(self.down2)\n",
        "      self.down3 = torch.utils.checkpoint(self.down3)\n",
        "      self.down4 = torch.utils.checkpoint(self.down4)\n",
        "      self.up1 = torch.utils.checkpoint(self.up1)\n",
        "      self.up2 = torch.utils.checkpoint(self.up2)\n",
        "      self.up3 = torch.utils.checkpoint(self.up3)\n",
        "      self.up4 = torch.utils.checkpoint(self.up4)\n",
        "      self.outc = torch.utils.checkpoint(self.outc)\n",
        "# The noising part\n",
        "def forward_diffusion(alphas_cumprod, x_start, t, noise):\n",
        "    batch_size, _, height, width = x_start.size()\n",
        "    # Expand alphas_cumprod[t] to match the shape of x_start and noise\n",
        "    alphas_cumprod_t = alphas_cumprod[t].view(batch_size, 1, 1, 1).expand(batch_size, 1, height, width).to(x_start.device)\n",
        "    return torch.sqrt(alphas_cumprod_t) * x_start + torch.sqrt(1 - alphas_cumprod_t) * noise\n"
      ],
      "metadata": {
        "id": "lZsJJ7OhfHJP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop"
      ],
      "metadata": {
        "id": "Y_fIJZsvj_am"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "def get_alphas_cumprod(T):\n",
        "    betas = np.linspace(1e-4, 0.02, T)\n",
        "    alphas = 1 - betas\n",
        "    alphas_cumprod = np.cumprod(alphas)\n",
        "    return alphas_cumprod\n",
        "T = 1000  # Number of diffusion steps\n",
        "def train_model(train_loader,epoch_counts=30,lr=1e-3,weights_save_path = \"./weights\"):\n",
        "  alphas_cumprod = get_alphas_cumprod(T)\n",
        "  alphas_cumprod = torch.tensor(alphas_cumprod).float().to(device)\n",
        "\n",
        "  model = DiffusionUNet(in_channels=1,n_classes=1,bilinear=False)\n",
        "  model.to(device)\n",
        "\n",
        "  optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "  loss_criterion = nn.MSELoss()\n",
        "# Generate alpha values to noise on each timestep\n",
        "  model.train()\n",
        "  for epoch in range(epoch_counts):\n",
        "    epoch_start_time = time.time()  # Start time before epoch begins\n",
        "    epoch_loss = 0\n",
        "    for batch_idx, (data,_) in enumerate(train_loader):\n",
        "      data = data.view(-1,1,28,28).to(device) # Reshape and\n",
        "      t = torch.randint(0,T,(data.size(0),),dtype=torch.long,device=device)\n",
        "      noise = torch.randn_like(data,device=device)\n",
        "      x_t = forward_diffusion(alphas_cumprod,data,t,noise)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      predicted_noise = model(x_t)\n",
        "      if torch.isnan(predicted_noise).any():\n",
        "        print(f\"NaNs detected in model output at batch {batch_idx}\")\n",
        "        break\n",
        "      loss = loss_criterion(predicted_noise, noise)\n",
        "      loss.backward()\n",
        "      # Update model weights\n",
        "      optimizer.step()\n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "\n",
        "    epoch_end_time = time.time()\n",
        "    formatted_time = str(datetime.timedelta(seconds=int(epoch_end_time - epoch_start_time)))\n",
        "\n",
        "    print(f'Epoch {epoch+1} | Time Taken : {formatted_time} | Loss: {epoch_loss/len(train_loader.dataset):.4f}')\n",
        "    torch.save(model.state_dict(),f\"weights_{epoch}.pth\")\n",
        "\n",
        "  # Save weights\n",
        "\n",
        "train_model(train_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "IoymBK6bkDyL",
        "outputId": "e720d69d-086c-4d95-d7c6-7f44651a79bc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Epoch 1 | Time Taken : 0:01:00 | Loss: 0.0001\n",
            "Epoch 2 | Time Taken : 0:00:59 | Loss: 0.0000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-c9cec48af6c8>\u001b[0m in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0;31m# Save weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-c9cec48af6c8>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_loader, epoch_counts, lr, weights_save_path)\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m       \u001b[0mpredicted_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_noise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"NaNs detected in model output at batch {batch_idx}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Model Weights\n",
        "T = 1000\n",
        "\n",
        "def load_model(model, weights_path):\n",
        "    model.load_state_dict(torch.load(weights_path))\n",
        "    model.eval()\n",
        "    print(f'Model weights loaded from {weights_path}')\n",
        "\n",
        "# Load the model weights\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = DiffusionUNet(in_channels=1, n_classes=1, bilinear=False).to(device)\n",
        "load_model(model, \"weights_1.pth\")\n",
        "\n",
        "# Get alphas_cumprod and move it to the correct device\n",
        "alphas_cumprod = get_alphas_cumprod(T)\n",
        "alphas_cumprod = torch.tensor(alphas_cumprod).float().to(device)\n",
        "\n",
        "\n",
        "epsilon = 1e-8  # Small constant to avoid division by zero\n",
        "def check_model_weights(model):\n",
        "    for name, param in model.named_parameters():\n",
        "        if torch.isnan(param).any():\n",
        "            print(f\"NaNs found in weights: {name}\")\n",
        "\n",
        "check_model_weights(model)\n",
        "\n",
        "\n",
        "def sample(model, num_samples=3):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x = torch.randn(num_samples, 1, 28, 28, device=device)  # Start with random noise\n",
        "        for t in reversed(range(T)):\n",
        "            predicted_noise = model(x)\n",
        "\n",
        "            # Debugging: Check for NaNs in predicted noise\n",
        "            if torch.isnan(predicted_noise).any():\n",
        "                print(f\"Model output contains NaNs at timestep {t}!\")\n",
        "                print(\"Predicted noise:\", predicted_noise)\n",
        "                return None\n",
        "\n",
        "            sqrt_alpha = torch.sqrt(alphas_cumprod[t] + epsilon)  # Use torch.sqrt\n",
        "            x = (x - predicted_noise) / sqrt_alpha\n",
        "\n",
        "            # Check for NaNs in the updated tensor\n",
        "            if torch.isnan(x).any():\n",
        "                print(f\"Updated tensor contains NaNs at timestep {t}!\")\n",
        "                print(\"Current tensor:\", x)\n",
        "                return None\n",
        "    return x.cpu()\n",
        "\n",
        "# Generate new images\n",
        "generated_images = sample(model)\n",
        "\n",
        "# # Ensure that there are no NaNs in the generated images\n",
        "# if generated_images is not None and torch.isnan(generated_images).any():\n",
        "#     print(\"Generated images contain NaNs!\")\n",
        "# else:\n",
        "#     print(\"Generated images are\", generated_images)\n",
        "\n",
        "#     # Plot the generated images\n",
        "#     plt.figure(figsize=(8, 8))\n",
        "#     for i in range(len(generated_images)):  # Use len(generated_images) to match the number of images generated\n",
        "#         plt.subplot(1, len(generated_images), i + 1)\n",
        "#         plt.imshow(generated_images[i].reshape(28, 28), cmap='gray')\n",
        "#         plt.axis('off')\n",
        "#     plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFStbwe-udSR",
        "outputId": "b592fb7f-42b8-4e2a-f56a-6c1dc706a7c3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model weights loaded from weights_1.pth\n",
            "Model output contains NaNs at timestep 980!\n",
            "Predicted noise: tensor([[[[nan, nan, nan,  ..., nan, nan, nan],\n",
            "          [nan, nan, nan,  ..., nan, nan, nan],\n",
            "          [nan, nan, nan,  ..., nan, nan, nan],\n",
            "          ...,\n",
            "          [nan, nan, nan,  ..., nan, nan, nan],\n",
            "          [nan, nan, nan,  ..., nan, nan, nan],\n",
            "          [nan, nan, nan,  ..., nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan,  ..., nan, nan, nan],\n",
            "          [nan, nan, nan,  ..., nan, nan, nan],\n",
            "          [nan, nan, nan,  ..., nan, nan, nan],\n",
            "          ...,\n",
            "          [nan, nan, nan,  ..., nan, nan, nan],\n",
            "          [nan, nan, nan,  ..., nan, nan, nan],\n",
            "          [nan, nan, nan,  ..., nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan,  ..., nan, nan, nan],\n",
            "          [nan, nan, nan,  ..., nan, nan, nan],\n",
            "          [nan, nan, nan,  ..., nan, nan, nan],\n",
            "          ...,\n",
            "          [nan, nan, nan,  ..., nan, nan, nan],\n",
            "          [nan, nan, nan,  ..., nan, nan, nan],\n",
            "          [nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K1fRHXJu28fH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}